# üöÄ Provisionamento e Configura√ß√£o de Airflow em EC2 Linux com Baixo Custo

![AWS](https://img.shields.io/badge/AWS-EC2-orange?style=for-the-badge&logo=amazon-aws)
![Linux](https://img.shields.io/badge/Linux-Ubuntu-E95420?style=for-the-badge&logo=linux)
![Airflow](https://img.shields.io/badge/Apache%20Airflow-2.10-blue?style=for-the-badge&logo=apache-airflow)
![Python](https://img.shields.io/badge/Python-3.12-3776AB?style=for-the-badge&logo=python)

## üìã Sobre o Projeto

Este projeto consiste na cria√ß√£o de um **ambiente de desenvolvimento robusto na nuvem (AWS)** para orquestra√ß√£o de pipelines de dados. O objetivo principal foi viabilizar o uso do **Apache Airflow** em uma inst√¢ncia EC2 de baixo custo, superando limita√ß√µes de hardware atrav√©s de otimiza√ß√µes no sistema operacional.

### Arquitetura e Tecnologias
* **Cloud Compute:** AWS EC2 (Tier T3/T2).
* **OS:** Linux Ubuntu 24.04 LTS.
* **Orquestra√ß√£o:** Apache Airflow 2.10 (Standalone Mode).
* **Conectividade:** SSH Remoto (VSCode Integration).
* **Otimiza√ß√£o:** Gerenciamento de Swap Memory e Virtual Environments (venv).


## Desafio T√©cnico: Superar o "Out of Memory"

**O Problema:** A execu√ß√£o simult√¢nea do *Webserver* e do *Scheduler* do Airflow exige uma quantidade de RAM que excede os limites das inst√¢ncias Free Tier (t2.micro/t3.micro), resultando em travamentos constantes (OOM Kills).

**A Solu√ß√£o de Engenharia:**
Implementa√ß√£o de **Swap Memory** (Mem√≥ria Virtual) no Linux. Alocamos 2GB de disco para atuar como extens√£o da RAM, suportando os picos de processamento durante a inicializa√ß√£o dos servi√ßos.

```bash
sudo fallocate -l 2G /swapfile
sudo chmod 600 /swapfile
sudo mkswap /swapfile
sudo swapon /swapfile
```

**Resultado:** Um ambiente est√°vel, funcional e economicamente vi√°vel para estudos e POCs.

>‚ö†Ô∏è Vale ressaltar que esta abordagem foca na otimiza√ß√£o de recursos para estudos utilizando o Free Tier da AWS. Em ambientes de produ√ß√£o com inst√¢ncias de maior capacidade (ex: t3.medium+), o uso de Swap √© opcional, mas o pipeline de instala√ß√£o e configura√ß√£o do Airflow descrito abaixo permanece id√™ntico.

## Guia de Implementa√ß√£o (Passo a Passo)

### 1. Provisionamento da Infraestrutura
Cria√ß√£o da inst√¢ncia EC2 e configura√ß√£o do Security Group (libera√ß√£o da porta 8080 e 22).

- Launch da inst√¢ncia ec2
<p align="center"> <img src="img/aws-ec2-instance.png" alt="Inst√¢ncia EC2 AWS" width="600"/> </p>

---

### 2. Acesso ssh remoto pelo bash

<p align="center">
  <img src="img/login-aws.png" />
</p>

---

### 3. Prepara√ß√£o do Sistema Operacional

#### Atualizar lista de pacotes

```bash
sudo apt update && sudo apt upgrade -y
```

#### Instalar gerenciador de pacotes Python e SQLite

```bash
sudo apt install -y python3-pip sqlite3
```

#### Instalar depend√™ncias para Postgres (caso decida escalar futuramente)

```bash
sudo apt-get install -y libpq-dev
```
#### Instalar suporte a ambientes virtuais (Python 3.12 - Default Ubuntu 24.04)

```bash
sudo apt install -y python3.12-venv
```

---

### 4. Configura√ß√£o de Swap 
- Etapa crucial para evitar travamentos em inst√¢ncias pequenas

```bash
sudo fallocate -l 2G /swapfile
sudo chmod 600 /swapfile
sudo mkswap /swapfile
sudo swapon /swapfile
```

- **sudo fallocate -l 2G /swapfile**

    - **O que faz**: Cria um arquivo vazio chamado swapfile na raiz do sistema.

- **sudo chmod 600 /swapfile**

  - **O que faz**: Altera as permiss√µes de seguran√ßa desse arquivo.

- **sudo mkswap /swapfile**

  - **O que faz:** "Formata" o arquivo.

- **sudo swapon /swapfile**

  - **O que faz:** Ativa a mem√≥ria.

>‚ö†Ô∏è Swap √© uma solu√ß√£o paliativa, funciona para estudos, mas n√£o √© recomendado para ambientes de produ√ß√£o devido √† perda de performance e desgaste do disco.

---

### 5. Configura√ß√£o do Ambiente Python
- Isolamos as depend√™ncias do projeto para evitar conflitos com o sistema operacional

```bash
python3 -m venv .venv
source .venv/bin/activate
```

---

### 6. **Instalar o Apache Airflow**

- Instala√ß√£o via pip utilizando as constraints oficiais para garantir estabilidade das vers√µes

#### Define a vers√£o do Airflow e do Python para buscar as constraints corretas automaticamente

```bash
AIRFLOW_VERSION=2.10.0
PYTHON_VERSION="$(python --version | cut -d " " -f 2 | cut -d "." -f 1-2)"
CONSTRAINT_URL="https://raw.githubusercontent.com/apache/airflow/constraints-${AIRFLOW_VERSION}/constraints-${PYTHON_VERSION}.txt"

```

#### Instala√ß√£o 
- tamb√©m instalamos o Celery, que √© uma biblioteca usada para rodar tarefas em paralelo (distribu√≠das) e pode ser usada caso precisemos de escala e migra√ß√£o.

```bash
pip install "apache-airflow[celery]==${AIRFLOW_VERSION}" --constraint "${CONSTRAINT_URL}"
```

---


### 7. **Inicializar o Banco de Dados do Airflow**

- Migra o banco de dados do Airflow para garantir que todas as tabelas necess√°rias sejam criadas

```bash
airflow db migrate
```

### 8. **Criar um Usu√°rio Administrador**

Criar um usu√°rio administrador para acessar a interface web do Airflow

```bash
airflow users create \
    --username admin \
    --firstname Peter \
    --lastname Parker \
    --role Admin \
    --email spiderman@superhero.org
```

- digite uma senha quando solicitado.

--- 
### 9. Execu√ß√£o do Servi√ßo
- Para ambientes de desenvolvimento, utilizamos o modo standalone que sobe todos os componentes (Webserver, Scheduler, Triggerer) de uma vez.

```bash
airflow standalone
```

>‚ö†Ô∏è Nota: O modo standalone utiliza SQLite e SequentialExecutor. Ele n√£o permite paralelismo real de tarefas, sendo ideal apenas para desenvolvimento e estudos.

---

### 10. **Adicionar uma Nova DAG**


#### Desenvolvimento Remoto (VSCode)
- Configura√ß√£o do Remote SSH no VSCode para editar DAGs diretamente no servidor.

<p align="center"> <img src="img/vscode-dag.png" alt="VSCode Remoto" width="700"/> </p>



Para adicionar uma nova DAG:

1. **Criar o Arquivo da DAG:**

   Navegue at√© o diret√≥rio de DAGs do Airflow e crie um novo arquivo Python para a DAG:

   ```bash
   cd ~/airflow/dags
   ```

   **Exemplo de Conte√∫do da DAG:**

   ```python
   from airflow import DAG
   from airflow.operators.dummy import DummyOperator
   from datetime import datetime

   default_args = {
       'owner': 'airflow',
       'start_date': datetime(2024, 8, 26),
   }

   with DAG('minha_dag',
            default_args=default_args,
            schedule_interval='@daily',
            catchup=False) as dag:

       start = DummyOperator(task_id='start')

       start
   ```


## Acessando o Ambiente
### Interface Web
- Com a porta 8080 liberada no Security Group da AWS, acesse pelo DNS p√∫blico: http://<ec2-public-dns>:8080

<p align="center"> <img src="img/airflow-dag.png" alt="Interface Airflow" width="700"/> </p>



# Pr√≥ximos Passos (Roadmap)
Para evoluir este projeto de um ambiente de estudos para um ambiente mais pr√≥ximo de produ√ß√£o, os seguintes passos foram mapeados:

[ ] Automatiza√ß√£o (Systemd): Configurar o Airflow para iniciar automaticamente caso a inst√¢ncia seja reiniciada (criar servi√ßos .service no Linux).

[ ] Banco de Dados Robusto: Substituir o backend SQLite pelo PostgreSQL (instalado localmente ou via Amazon RDS).

[ ] Paralelismo: Alterar o executor para LocalExecutor (permite rodar tarefas em paralelo).

[ ] Deploy de DAGs: Criar um fluxo de CI/CD (GitHub Actions) para enviar as DAGs do reposit√≥rio para a pasta /dags na EC2 automaticamente.

[ ] Containeriza√ß√£o: Migrar essa instala√ß√£o "bare metal" para Docker/Docker Compose.

---

**Criado por [Diego](https://github.com/diipdata)**  
diegop.freitas@gmail.com | [LinkedIn](https://linkedin.com/in/diegop-freitas) | [X/Twitter](https://x.com/diipdata)

*Feito com ‚òï e muitas linhas de c√≥digo (e alguns erros pelo caminho).*